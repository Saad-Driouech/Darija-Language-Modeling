{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Uncomment the next statement in case transformers module is not installed\n",
    "# !pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# Detect hardware, return appropriate distribution strategy\n",
    "try:\n",
    "    # TPU detection. No parameters necessary if TPU_NAME environment variable is\n",
    "    # set: this is always the case on Kaggle.\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "    print('Running on TPU ', tpu.master())\n",
    "except ValueError:\n",
    "    tpu = None\n",
    "\n",
    "if tpu:\n",
    "    tf.config.experimental_connect_to_cluster(tpu)\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "else:\n",
    "    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n",
    "    strategy = tf.distribute.get_strategy()\n",
    "\n",
    "print(\"REPLICAS: \", strategy.num_replicas_in_sync)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import TFAutoModel, AutoTokenizer, AutoModel\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Configuration\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 32 * strategy.num_replicas_in_sync\n",
    "# change max sequence length depending on how long data samples are\n",
    "MAX_LEN = 128\n",
    "PRETRAINED_MODEL = \"Set this to the name or path of any pretrained model\"\n",
    "DATA = \"Set this to the path of your data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining metrics\n",
    "\n",
    "from keras import backend as K\n",
    "\n",
    "def recall_m(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building CNN model.  Hyperparameters should be changed depending on your case\n",
    "\n",
    "def build_model_cnn(transformer, max_len=512):\n",
    "\n",
    "    inputs = tf.keras.layers.Input(shape=(max_len,), dtype=tf.int32)\n",
    "    sequence_output = transformer(inputs)[0]\n",
    "    pooled_output = tf.keras.layers.Dense(transformer.config.hidden_size,\n",
    "                          kernel_initializer=tf.keras.initializers.TruncatedNormal(stddev=transformer.config.initializer_range),\n",
    "                          activation=\"tanh\"\n",
    "                         )(sequence_output)\n",
    "    cnn = tf.keras.layers.Conv1D(128, 3,\n",
    "                                bias_regularizer=tf.keras.regularizers.L1L2(l1=0.0, l2=0.0),\n",
    "                                kernel_regularizer=tf.keras.regularizers.L1L2(l1=0.01, l2=0.0),\n",
    "                                activity_regularizer=tf.keras.regularizers.L1L2(l1=0.0, l2=0.0))(pooled_output)\n",
    "    max_pooling = tf.keras.layers.MaxPool1D(2)(cnn)\n",
    "    dropout = tf.keras.layers.Dropout(0.1)(max_pooling)\n",
    "    flatten = tf.keras.layers.Flatten()(dropout)\n",
    "    out = tf.keras.layers.Dense(1, \n",
    "                activation='sigmoid',\n",
    "                bias_regularizer=tf.keras.regularizers.L1L2(l1=0.0, l2=0.0),\n",
    "                kernel_regularizer=tf.keras.regularizers.L1L2(l1=0.0, l2=0.0),\n",
    "                activity_regularizer=tf.keras.regularizers.L1L2(l1=0.0, l2=0.01)\n",
    "               )(flatten)\n",
    "    model = tf.keras.models.Model(inputs=inputs, outputs=out)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building LSTM model. Hyperparameters should be changed depending on your case\n",
    "\n",
    "def build_model_lstm(transformer, max_len=512):\n",
    "\n",
    "    inputs = tf.keras.layers.Input(shape=(max_len,), dtype='int32')\n",
    "    sequence_output = transformer(inputs)[0]\n",
    "    pooled_output = tf.keras.layers.Dense(transformer.config.hidden_size,\n",
    "                          kernel_initializer=tf.keras.initializers.TruncatedNormal(stddev=transformer.config.initializer_range),\n",
    "                          activation=\"tanh\"\n",
    "                         )(sequence_output)\n",
    "    lstm = tf.keras.layers.LSTM(128, \n",
    "                                dropout=0.2,\n",
    "                                bias_regularizer=tf.keras.regularizers.L1L2(l1=0.0, l2=0.0),\n",
    "                                kernel_regularizer=tf.keras.regularizers.L1L2(l1=0.01, l2=0.0),\n",
    "                                activity_regularizer=tf.keras.regularizers.L1L2(l1=0.0, l2=0.0)\n",
    "                               )(pooled_output)\n",
    "    dropout2 = tf.keras.layers.Dropout(0.1)(lstm)\n",
    "    out = tf.keras.layers.Dense(1, \n",
    "                activation='sigmoid',\n",
    "                bias_regularizer=tf.keras.regularizers.L1L2(l1=0.0, l2=0.0),\n",
    "                kernel_regularizer=tf.keras.regularizers.L1L2(l1=0.0, l2=0.0),\n",
    "                activity_regularizer=tf.keras.regularizers.L1L2(l1=0.0, l2=0.01)\n",
    "               )(dropout2)\n",
    "    model = tf.keras.models.Model(inputs=inputs, outputs=out)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building CNN-LSTM model. Hyperparameters should be changed depending on your case\n",
    "\n",
    "def build_model_cnn_lstm(transformer, max_len=512):\n",
    "\n",
    "\n",
    "    inputs = tf.keras.layers.Input(shape=(max_len,), dtype=tf.int32)\n",
    "    sequence_output = transformer(inputs)[0]\n",
    "    pooled_output = tf.keras.layers.Dense(transformer.config.hidden_size,\n",
    "                          kernel_initializer=tf.keras.initializers.TruncatedNormal(stddev=transformer.config.initializer_range),\n",
    "                          activation=\"tanh\"\n",
    "                         )(sequence_output)\n",
    "    cnn = tf.keras.layers.Conv1D(128, 2,\n",
    "                                bias_regularizer=tf.keras.regularizers.L1L2(l1=0.0, l2=0.0),\n",
    "                                kernel_regularizer=tf.keras.regularizers.L1L2(l1=0.01, l2=0.0),\n",
    "                                activity_regularizer=tf.keras.regularizers.L1L2(l1=0.0, l2=0.0)\n",
    "                                )(pooled_output)\n",
    "    max_pooling = tf.keras.layers.MaxPool1D(2)(cnn)\n",
    "    dropout = tf.keras.layers.Dropout(0.1)(max_pooling)\n",
    "    flatten = tf.keras.layers.Flatten()(dropout)\n",
    "    lstm = tf.keras.layers.LSTM(128,\n",
    "                                bias_regularizer=tf.keras.regularizers.L1L2(l1=0.0, l2=0.0),\n",
    "                                kernel_regularizer=tf.keras.regularizers.L1L2(l1=0.01, l2=0.0),\n",
    "                                activity_regularizer=tf.keras.regularizers.L1L2(l1=0.0, l2=0.0)\n",
    "                               )(flatten)\n",
    "    dropout2 = tf.keras.layers.Dropout(0.2)(lstm)\n",
    "    out = tf.keras.layers.Dense(1, \n",
    "                activation='sigmoid',\n",
    "                bias_regularizer=tf.keras.regularizers.L1L2(l1=0.0, l2=0.0),\n",
    "                kernel_regularizer=tf.keras.regularizers.L1L2(l1=0.0, l2=0.0),\n",
    "                activity_regularizer=tf.keras.regularizers.L1L2(l1=0.0, l2=0.01)\n",
    "               )(dropout2)\n",
    "    model = tf.keras.models.Model(inputs=inputs, outputs=out)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "off_data = pd.read_excel(DATA)\n",
    "\n",
    "off_data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = off_data['text']\n",
    "y = off_data['label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regular_encode(texts, tokenizer, maxlen=200):\n",
    "    \n",
    "    # encode the word to vector of integer\n",
    "    enc_di = tokenizer.batch_encode_plus(\n",
    "        texts, \n",
    "        return_token_type_ids=False,\n",
    "        pad_to_max_length=True,\n",
    "        is_split_into_words=False,\n",
    "        max_length=maxlen\n",
    "    )\n",
    "    \n",
    "    return np.array(enc_di['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(PRETRAINED_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = regular_encode(list(X_train), tokenizer, maxlen = MAX_LEN)\n",
    "X_test = regular_encode(list(X_test), tokenizer, maxlen = MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the weights of each class\n",
    "\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Count samples per class\n",
    "classes_zero = off_data[off_data['label'] == 0]\n",
    "classes_one = off_data[off_data['label'] == 1]\n",
    "\n",
    "# Convert parts into NumPy arrays for weight computation\n",
    "zero_numpy = classes_zero['label'].to_numpy()\n",
    "one_numpy = classes_one['label'].to_numpy()\n",
    "all_together = np.concatenate((zero_numpy, one_numpy))\n",
    "unique_classes = np.unique(all_together)\n",
    "\n",
    "# Compute weights\n",
    "weights = compute_class_weight( \"balanced\", classes = unique_classes,y= all_together)\n",
    "\n",
    "weights = {l:c for l,c in zip(np.unique(all_together), weights)}\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparing the training and test datasets for the model\n",
    "\n",
    "train_dataset = (\n",
    "    tf.data.Dataset \n",
    "    .from_tensor_slices((X_train, y_train)) \n",
    "    .repeat()\n",
    "    .shuffle(2048)\n",
    "    .batch(BATCH_SIZE)\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE) \n",
    ")\n",
    "\n",
    "test_dataset = (\n",
    "    tf.data.Dataset \n",
    "    .from_tensor_slices((X_test, y_test)) \n",
    "    .batch(BATCH_SIZE) \n",
    "    .cache()\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE) \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_size = X_train.shape[0]\n",
    "steps_per_epoch = int(train_data_size / BATCH_SIZE)\n",
    "num_train_steps = steps_per_epoch * EPOCHS\n",
    "initial_learning_rate=3e-5\n",
    "\n",
    "# Use polynomial decay for the learning rate\n",
    "polynomial_decay = tf.keras.optimizers.schedules.PolynomialDecay(\n",
    "    initial_learning_rate=initial_learning_rate,\n",
    "    end_learning_rate=0,\n",
    "    decay_steps=num_train_steps)\n",
    "\n",
    "optimizer = Adam(\n",
    "    learning_rate = polynomial_decay)\n",
    "\n",
    "x = tf.linspace(0, num_train_steps, 1001)\n",
    "y = [linear_decay(xi) for xi in x]\n",
    "plt.plot(x,y)\n",
    "plt.xlabel('Train step')\n",
    "plt.ylabel('Learning rate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    #take the encoder results of bert from transformers and use it as an input in the NN model\n",
    "    transformer_layer = TFAutoModel.from_pretrained(PRETRAINED_MODEL, hidden_dropout_prob=0.1, attention_probs_dropout_prob=0.1, from_pt=True)\n",
    "    \n",
    "    # change the function call according to which experiment you want to try: CNN, LSTM, or CNN-LSTM\n",
    "    model = build_model_cnn_lstm(transformer_layer, max_len=MAX_LEN)\n",
    "    \n",
    "    model.compile(optimizer=optimizer, \n",
    "                  loss='binary_crossentropy', \n",
    "                  metrics=[recall_m, precision_m, f1_m, tf.keras.metrics.BinaryAccuracy(name=\"binary_accuracy\", dtype=None, threshold=0.5)]\n",
    "                 )\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = X_train.shape[0] // BATCH_SIZE\n",
    "\n",
    "# EarlyStopping callback is used to stop the training once the validation loss goes up (patience=1)\n",
    "es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=1)\n",
    "\n",
    "# ModelChekpoint callback is used to save the best model depending on validation f1_score (highest value)\n",
    "# model.h5 is the path of the checkpoint in case you uncomment the last statement\n",
    "mc = tf.keras.callbacks.ModelCheckpoint('model.h5', monitor='val_f1_m', mode='max', verbose=1, save_best_only=True)\n",
    "\n",
    "train_history = model.fit(train_dataset, steps_per_epoch=n_steps,\n",
    "                          validation_data=test_dataset,\n",
    "                          epochs=10,\n",
    "                          class_weight=weights,\n",
    "                          callbacks=[es, mc])\n",
    "\n",
    "# Uncomment the next statement if you want to save the checkpoint\n",
    "# model.save(\"myModel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the evolution of the training and validation losses\n",
    "\n",
    "history = train_history\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "\n",
    "epochs = range(0,len(loss))\n",
    "\n",
    "plt.plot(epochs, np.array(loss), label='Loss')\n",
    "plt.plot(epochs, np.array(val_loss), label='Validation Loss')\n",
    "\n",
    "plt.title('Training and Validation Losses')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Value')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the evolution of the training and validation accuracy and f1 scores\n",
    "\n",
    "history = train_history\n",
    "acc = history.history['binary_accuracy']\n",
    "f1 = history.history['f1_m']\n",
    "acc_val = history.history['val_binary_accuracy']\n",
    "f1_val = history.history['val_f1_m']\n",
    "epochs = range(0,len(acc))\n",
    "\n",
    "plt.plot(epochs, np.array(acc), label='Accuracy')\n",
    "plt.plot(epochs, np.array(acc_val), label='Validation Accuracy')\n",
    "plt.plot(epochs, np.array(f1), label='F1')\n",
    "plt.plot(epochs, np.array(f1_val), label='Validation F1')\n",
    "\n",
    "plt.title('Training and Validation Accuracy and F1')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Value')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
